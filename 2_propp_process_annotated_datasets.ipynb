{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-11T15:58:57.741536293Z",
     "start_time": "2026-02-11T15:58:56.646131771Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:58:57.790191215Z",
     "start_time": "2026-02-11T15:58:57.742298553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_files_directory = \"bamman2014_similarity_triplets/txt_files\"\n",
    "input_files_directory = \"CharaSim-fr/txt_files\"\n",
    "\n",
    "output_files_directory = input_files_directory.replace(\"txt_files\", \"propp_processed_files\")\n",
    "os.makedirs(output_files_directory, exist_ok=True)"
   ],
   "id": "36d17edc5cb88602",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:58:57.847794146Z",
     "start_time": "2026-02-11T15:58:57.791245012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "characters_aliases_dict_path = input_files_directory.replace(\"txt_files\", \"characters_aliases_dict.json\")\n",
    "\n",
    "with open(characters_aliases_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    characters_aliases_dict = json.load(f)\n",
    "print(len(characters_aliases_dict))"
   ],
   "id": "900552bfab33f1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:58:57.955821009Z",
     "start_time": "2026-02-11T15:58:57.871577165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "txt_files = sorted(p.stem for p in Path(input_files_directory).iterdir() if p.suffix == \".txt\")\n",
    "entities_files = sorted(p.stem for p in Path(output_files_directory).iterdir() if p.suffix == \".entities\")\n",
    "\n",
    "print(f\"Text files: {len(txt_files):,}\")\n",
    "print(f\"Entities files: {len(entities_files):,}\")"
   ],
   "id": "d2e242e684e7bf04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files: 163\n",
      "Book files: 163\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:59:08.072765667Z",
     "start_time": "2026-02-11T15:58:57.958197615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from propp_fr import load_models, load_text_file, generate_tokens_df, load_tokenizer_and_embedding_model, get_embedding_tensor_from_tokens_df, generate_entities_df, add_features_to_entities, perform_coreference, extract_attributes, save_tokens_df, save_entities_df\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "unprocessed_files = [file for file in txt_files if file not in entities_files]\n",
    "print(f\"Unprocessed Files: {len(unprocessed_files):,}\")\n",
    "\n",
    "spacy_model, mentions_detection_model, coreference_resolution_model = load_models(\n",
    "    spacy_model_name='fr_dep_news_trf',\n",
    "    mentions_detection_model_name='AntoineBourgois/propp-fr_NER_camembert-large_PER', coreference_resolution_model_name='AntoineBourgois/propp-fr_coreference-resolution_camembert-large_PER')\n",
    "tokenizer, embedding_model = load_tokenizer_and_embedding_model(mentions_detection_model[\"base_model_name\"])\n",
    "\n",
    "for file_name in tqdm(unprocessed_files, desc=\"Processing .txt Files\"):\n",
    "    print(f\"Processing: {file_name}...\")\n",
    "    text_content = load_text_file(file_name, input_files_directory)\n",
    "    tokens_df = generate_tokens_df(text_content, spacy_model, max_char_sentence_length=25000)\n",
    "    tokens_embedding_tensor = get_embedding_tensor_from_tokens_df(\n",
    "        text_content,\n",
    "        tokens_df,\n",
    "        tokenizer,\n",
    "        embedding_model,\n",
    "    )\n",
    "    torch.save(tokens_embedding_tensor, os.path.join(output_files_directory, file_name + \".tokens_embedding_tensor\"))\n",
    "\n",
    "    entities_df = generate_entities_df(\n",
    "        tokens_df,\n",
    "        tokens_embedding_tensor,\n",
    "        mentions_detection_model,\n",
    "    )\n",
    "\n",
    "    entities_df = add_features_to_entities(entities_df, tokens_df)\n",
    "\n",
    "    characters_alias_list = characters_aliases_dict[file_name] if file_name in characters_aliases_dict else None\n",
    "    print(characters_alias_list)\n",
    "\n",
    "    entities_df = perform_coreference(\n",
    "        entities_df,\n",
    "        tokens_embedding_tensor,\n",
    "        coreference_resolution_model,\n",
    "        propagate_coref=True,\n",
    "        rule_based_postprocess=False,\n",
    "        characters_alias_list=characters_alias_list,\n",
    "    )\n",
    "\n",
    "    tokens_df = extract_attributes(entities_df, tokens_df)\n",
    "\n",
    "    save_tokens_df(tokens_df, file_name, output_files_directory)\n",
    "    save_entities_df(entities_df, file_name, output_files_directory)"
   ],
   "id": "e27b68c1acb70c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "propp_fr package loaded successfully.\n",
      "Unprocessed Files: 0\n",
      "Loading models...\n",
      "CUDA is required, Spacy model should run on GPU.\n",
      "Model Loaded Successfully from local path: /home/antoine/Bureau/character_attributes_classification/AntoineBourgois/propp-fr_NER_camembert-large_PER/final_model.pkl\n",
      "Model Loaded Successfully from local path: /home/antoine/Bureau/character_attributes_classification/AntoineBourgois/propp-fr_coreference-resolution_camembert-large_PER/final_model\n",
      "\n",
      "Models Loaded Successfully:\n",
      "Spacy: fr_dep_news_trf\n",
      "Mentions Detection: AntoineBourgois/propp-fr_NER_camembert-large_PER\n",
      "Coreference Resolution: AntoineBourgois/propp-fr_coreference-resolution_camembert-large_PER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertModel were not initialized from the model checkpoint at almanach/camembert-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder model: almanach/camembert-large\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing .txt Files: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "664797b222bd42abaa6ffdb16a8f7d3d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Assign Character Name to COREF chain",
   "id": "fec6cab819e8a75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:59:08.286679294Z",
     "start_time": "2026-02-11T15:59:08.144390698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files_directory = \"bamman2014_similarity_triplets/propp_processed_files\"\n",
    "files_directory = \"CharaSim-fr/propp_processed_files\"\n",
    "\n",
    "characters_aliases_dict_path = files_directory.replace(\"propp_processed_files\", \"characters_aliases_dict.json\")\n",
    "\n",
    "with open(characters_aliases_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    characters_aliases_dict = json.load(f)\n",
    "print(len(characters_aliases_dict))"
   ],
   "id": "4481cb114ae7e7da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:59:17.436556724Z",
     "start_time": "2026-02-11T15:59:08.287739674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from propp_fr import load_entities_df, save_entities_df\n",
    "\n",
    "entities_files = sorted(p.stem for p in Path(files_directory).iterdir() if p.suffix == \".entities\")\n",
    "\n",
    "for file_name in tqdm(entities_files, desc=\"Processing .txt Files\"):\n",
    "    entities_df = load_entities_df(file_name, files_directory)\n",
    "    entities_df[\"COREF_name\"] = entities_df[\"COREF\"].astype(str)\n",
    "    characters_alias_list = characters_aliases_dict[file_name] if file_name in characters_aliases_dict else None\n",
    "\n",
    "    if characters_alias_list is not None:\n",
    "        for char_name, char_aliases in characters_alias_list.items():\n",
    "            char_COREF = entities_df[entities_df[\"text\"]==char_aliases[0]]\n",
    "            if len(char_COREF) > 0:\n",
    "                char_COREF = char_COREF[\"COREF\"].tolist()[0]\n",
    "                entities_df.loc[entities_df[entities_df[\"COREF\"]==char_COREF].index, \"COREF_name\"] = char_name\n",
    "\n",
    "    save_entities_df(entities_df, file_name, files_directory)"
   ],
   "id": "fc0e8520e8fd2ab5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing .txt Files:   0%|          | 0/163 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e691b562248b4d02be8f064537f8d22d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
